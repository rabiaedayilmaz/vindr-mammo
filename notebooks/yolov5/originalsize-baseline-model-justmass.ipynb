{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7650205,"sourceType":"datasetVersion","datasetId":4459722},{"sourceId":7703577,"sourceType":"datasetVersion","datasetId":4497192}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, random, shutil\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nfrom typing import List, Dict\n\nimport cv2\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\n\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_PATH = \"/kaggle/input/original-size-vindr-mammo/images_processed_cv2_dicomsdl_originalsize\"\n\nMETA_DATA = \"/kaggle/input/vindr-mammo-annotations/metadata.csv\"\nBREAST_LEVEL_DATA = \"/kaggle/input/vindr-mammo-annotations/breast-level_annotations.csv\"\nFINDING_ANNOTATIONS_DATA = \"/kaggle/input/vindr-mammo-annotations/finding_annotations.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df = pd.read_csv(FINDING_ANNOTATIONS_DATA)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def string_to_list(df_col: pd.core.series.Series = annotations_df[\"finding_categories\"]) -> List[List[str]]:\n    new_df_col = []\n    string_to_list = lambda string: [elem.strip(\"\\\"' \") for elem in string.strip(\"[]\").split(\",\")]\n    for cat in df_col:\n        new_df_col.append(string_to_list(cat))\n    return new_df_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#annotations_df[\"finding_categories\"] = string_to_list()\nannotations_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_mass = annotations_df[annotations_df.finding_categories == \"['Mass']\"]\nselected_mass.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_path_col(df: pd.core.frame.DataFrame = selected_mass) -> List[str]:\n    path_col = []\n    for idx, data_row in tqdm(df.iterrows(), total=len(df)):\n        study_id = str(data_row[\"study_id\"])\n        image_id = str(data_row[\"image_id\"])\n        patient_dir =  os.path.join(ROOT_PATH, study_id)\n        image_name = image_id + \".png\"\n        image_path = os.path.join(patient_dir, image_name)\n        path_col.append(image_path)\n    return path_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_mass[\"image_path\"] = create_path_col()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_mass.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_mass.split.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are images that do not have any annotations, drop them for model\nprint(len(selected_mass[selected_mass.xmin.isna()].index) == sum(selected_mass[selected_mass.xmin.isna()].index == selected_mass[selected_mass.ymin.isna()].index))\nprint(len(selected_mass[selected_mass.xmax.isna()].index) == sum(selected_mass[selected_mass.xmax.isna()].index == selected_mass[selected_mass.ymax.isna()].index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BIRADS 1-2\nselected_mass_benign = selected_mass[selected_mass.xmin.isna()].reset_index(drop=True)\nselected_mass_benign.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are no benign 1-2 birads\nselected_mass_benign.breast_birads.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BIRADS 3-4-5\nselected_mass_malignant = selected_mass[~selected_mass.xmin.isna()].reset_index(drop=True)\nselected_mass_malignant.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_mass_malignant.finding_birads.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_mass_malignant.breast_birads.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_mass_malignant.finding_birads.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert/scale/adjust annotation points to current one\noriginal_height, original_width = selected_mass[\"height\"][0], selected_mass[\"width\"][0]\n\nrescaled_xmin = int(selected_mass[\"xmin\"].iloc[0])\nrescaled_ymin = int(selected_mass[\"ymin\"].iloc[0])\nrescaled_xmax = int(selected_mass[\"xmax\"].iloc[0])\nrescaled_ymax = int(selected_mass[\"ymax\"].iloc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xx = (rescaled_xmin + rescaled_xmax) / 2\nyy = (rescaled_ymin + rescaled_ymax) / 2\nww = rescaled_xmax - rescaled_xmin\nhh = rescaled_ymax - rescaled_ymin\n\n(xx - ww / 2, yy - hh / 2),(xx + ww / 2, yy + hh / 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df = pd.read_csv(\"/kaggle/input/vindr-mammo-annotations/metadata.csv\")\nmeta_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df = selected_mass.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert/scale/adjust annotation points to current one\noriginal_height, original_width = annotations_df[\"height\"][0], annotations_df[\"width\"][0]\n\nrescaled_xmin = int(annotations_df[\"xmin\"].iloc[0])\nrescaled_ymin = int(annotations_df[\"ymin\"].iloc[0])\nrescaled_xmax = int(annotations_df[\"xmax\"].iloc[0])\nrescaled_ymax = int(annotations_df[\"ymax\"].iloc[0])\nxx = (rescaled_xmin + rescaled_xmax) // 2\nyy = (rescaled_ymin + rescaled_ymax) // 2\nww = rescaled_xmax - rescaled_xmin\nhh = rescaled_ymax - rescaled_ymin\n\n# visualize a sample image and annotation\nsample = plt.imread(annotations_df[\"image_path\"][0])\nsample_bgr = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n\n# Draw rectangle around the annotated area\ncv2.rectangle(sample_bgr,\n              (int(xx - (ww / 2)), int(yy - (hh / 2))),\n              (int(xx + (ww / 2)), int(yy + (hh / 2))),\n              (255, 0, 0), 5)\n\n# Add text to the image\ncv2.putText(sample_bgr, annotations_df[\"finding_birads\"][0], (rescaled_xmin-15, rescaled_ymin-35),\n            cv2.FONT_HERSHEY_COMPLEX, 2, (255, 200, 0), 3, cv2.LINE_AA)\n\n# Display the annotated image\nplt.imshow(cv2.cvtColor(sample_bgr, cv2.COLOR_BGR2RGB), cmap=\"bone\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef add_points(df: pd.core.frame.DataFrame = annotations_df):\n    x_centers, y_centers = [], []\n    ann_widths, ann_heights = [], []\n    \n    for i in range(df.shape[0]):\n        original_height, original_width = df[\"height\"][i], df[\"width\"][i]\n\n        rescaled_xmin = df[\"xmin\"].iloc[i]\n        rescaled_ymin = df[\"ymin\"].iloc[i]\n        rescaled_xmax = df[\"xmax\"].iloc[i]\n        rescaled_ymax = df[\"ymax\"].iloc[i]\n        \n        x_center = (rescaled_xmax + rescaled_xmin) / 2\n        y_center = (rescaled_ymax + rescaled_ymin) / 2\n        ann_width = (rescaled_xmax - rescaled_xmin)\n        ann_height = (rescaled_ymax - rescaled_ymin)\n\n        x_centers.append(x_center / original_width)\n        y_centers.append(y_center / original_height)\n        ann_widths.append(ann_width / original_width)\n        ann_heights.append(ann_height / original_height)\n    \n    rescaled_bboxes = {\n        \"x_centers\": x_centers,\n        \"y_centers\": y_centers,\n        \"ann_widths\": ann_widths,\n        \"ann_heights\": ann_heights,\n    }\n    \n    return rescaled_bboxes\n\nbboxes = add_points()\n\nannotations_df[\"x_centers\"] = bboxes[\"x_centers\"]\nannotations_df[\"y_centers\"] = bboxes[\"y_centers\"]\nannotations_df[\"ann_widths\"] = bboxes[\"ann_widths\"]\nannotations_df[\"ann_heights\"] = bboxes[\"ann_heights\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df_malignant.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize a sample image and annotation\nsample = plt.imread(annotations_df[\"image_path\"][0])\nsample_bgr = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n\noriginal_w = annotations_df[\"width\"][0]\noriginal_h = annotations_df[\"height\"][0]\n\n# Calculate rectangle coordinates\nxmin = int((annotations_df[\"x_centers\"][0] - annotations_df[\"ann_widths\"][0] / 2) * original_w) \nymin = int((annotations_df[\"y_centers\"][0] - annotations_df[\"ann_heights\"][0] / 2) * original_h)\nxmax = int((annotations_df[\"x_centers\"][0] + annotations_df[\"ann_widths\"][0] / 2) * original_w) \nymax = int((annotations_df[\"y_centers\"][0] + annotations_df[\"ann_heights\"][0] / 2) * original_h) \n\n# Draw rectangle around the annotated area\ncv2.rectangle(sample_bgr, (xmin, ymin), (xmax, ymax), (255, 0, 0), 3)\n\n# Add text to the image\ntext_x = int((annotations_df[\"x_centers\"][0] - annotations_df[\"ann_widths\"][0] / 2) * original_w - 15) \ntext_y = int((annotations_df[\"y_centers\"][0] - annotations_df[\"ann_heights\"][0] / 2) * original_h - 20) \ncv2.putText(sample_bgr, annotations_df[\"finding_birads\"][0], (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX, 2, (255, 200, 0), 3, cv2.LINE_AA)\n\nplt.imshow(sample_bgr, cmap=\"bone\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dictionary that maps class names to IDs\nclass_name_to_id_mapping = {\n                           \"BI-RADS 3\": 0,\n                           \"BI-RADS 4\": 1,\n                           \"BI-RADS 5\": 2,\n                           }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df[\"class_id\"] = [class_name_to_id_mapping[bf] for bf in annotations_df[\"breast_birads\"]] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df.finding_categories.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df[annotations_df.breast_birads.isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df.breast_birads.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df = annotations_df.drop_duplicates(subset=[\"image_id\"]).reset_index(drop=True)\ntmp_df[\"breast_birads\"] = annotations_df.groupby(\"image_id\")[\"breast_birads\"].agg(list).reset_index(drop=True)\ntmp_df[\"class_id\"] = annotations_df.groupby(\"image_id\")[\"class_id\"].agg(list).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df.shape, annotations_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouby image_id is important. there was a bug when grouped by study_id -> NaN bboxes...\ntmp_df[\"bboxes\"] = annotations_df.groupby(\"image_id\").apply(lambda x: x[[\"x_centers\", \"y_centers\", \"ann_widths\", \"ann_heights\"]].values.tolist()).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df[tmp_df.bboxes.isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can use one of them to label\ntmp_df[\"breast_birads\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_info_dict(df: pd.core.frame.DataFrame = annotations_df):\n    annotations = []\n    #os.makedirs('/kaggle/working/annotations', exist_ok=True)\n    \n    for i in range(df.shape[0]):\n        objects = []\n        \n        # Extracting bounding box information\n        x_center = df[\"x_centers\"].iloc[i]\n        y_center = df[\"y_centers\"].iloc[i]\n        ann_width = df[\"ann_widths\"].iloc[i]\n        ann_height = df[\"ann_heights\"].iloc[i]\n        class_id = df[\"class_id\"].iloc[i]\n        \n        # Calculate bounding box coordinates\n        xmin = x_center - ann_width / 2\n        ymin = y_center - ann_height / 2\n        xmax = x_center + ann_width / 2\n        ymax = y_center + ann_height / 2\n        \n        # Constructing object dictionary\n        obj = {\n            \"bbox\": [[x_center, y_center, ann_width, ann_height]],\n            #\"bbox\": [[x_center, y_center, ann_width, ann_height]],\n            \"class_id\": [class_id],\n        }\n        objects.append(obj)\n        \n        # Extracting file name\n        file_name = \"/\".join(df[\"image_path\"][i].split(\"/\")[5:])\n        \n        # Extracting image size\n        img_size = (df[\"height\"].iloc[0], df[\"width\"].iloc[0], 3)  # Assuming 3 channels\n        \n        # Constructing annotation dictionary\n        annotation = {\n            \"objects\": objects,\n            \"file_name\": file_name,\n            \"image_size\": img_size\n        }\n        \n        annotations.append(annotation)\n    \n    return annotations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annnotations = create_info_dict()\nlen(annnotations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annnotations[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annnotations[42]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import shutil\n#shutil.rmtree(\"/kaggle/working/annotations\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_yolov5(info_dict: Dict[str, List[Dict[str, any]]] = annnotations) -> None:\n    \n    # Create directory if it doesn't exist\n    save_directory = \"/kaggle/working/annotations\"\n    if not os.path.exists(save_directory):\n        os.makedirs(save_directory)\n    \n    for patient in tqdm(info_dict, total=len(info_dict)):\n        # For each bounding box\n        print_buffer = []  # Initialize print_buffer for each patient\n\n        objects = patient[\"objects\"][0]\n        img_size = patient[\"image_size\"]\n        file_name = patient[\"file_name\"]\n        \n        for bbox, category in zip(objects[\"bbox\"], objects[\"class_id\"]):\n            b_center_x = bbox[0]\n            b_center_y = bbox[1]\n            b_width = bbox[2]\n            b_height = bbox[3]\n            \n\n            # Write the bbox details to the file\n            print_buffer.append(\"{} {:.6f} {:.6f} {:.6f} {:.6f}\".format(category, b_center_x, b_center_y, b_width, b_height))\n\n        # Name of the file which we have to save\n        save_file_name = os.path.join(save_directory, file_name.replace(\"png\", \"txt\"))\n\n        # Save the annotation to disk\n        os.makedirs(os.path.dirname(save_file_name), exist_ok=True)\n        with open(save_file_name, \"w\") as f:\n            f.write(\"\\n\".join(print_buffer))\n\n# Call the function with your info_dict\nconvert_to_yolov5()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/annotations/cc2fa527db72082028747b8f14c3d578/9be8f8e91569f85bf8472c9eacc47753.txt\", \"r\") as f:\n    print(f.readlines())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_df_malignant.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check sanity !!!\n# visualize a sample image and annotation\nsample = plt.imread(annotations_df_malignant[\"image_path\"][0])\nsample_bgr = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n\noriginal_w = annotations_df_malignant[\"width\"][0]\noriginal_h = annotations_df_malignant[\"height\"][0]\n\nwith open(f\"/kaggle/working/annotations/{annotations_df_malignant.study_id[0]}/{annotations_df_malignant.image_id[0]}.txt\") as ann:\n    anns = ann.readlines()[0].split()\n    class_id = str(anns[0])\n    x_cen = float(anns[1])\n    y_cen = float(anns[2])\n    w = float(anns[3])\n    h = float(anns[4])\n\n# Calculate rectangle coordinates\nxmin = int((x_cen - (w / 2)) * original_w)\nymin = int((y_cen - (h / 2)) * original_h)\nxmax = int((x_cen + (w / 2)) * original_w)\nymax = int((y_cen + (h / 2)) * original_h)\n\n# Draw rectangle around the annotated area\ncv2.rectangle(sample_bgr, (xmin, ymin), (xmax, ymax), (255, 0, 0), 5)\n\n# Add text to the image\ntext_x = int(xmin - 15)\ntext_y = int(ymin + 35)\ncv2.putText(sample_bgr, annotations_df_malignant[\"finding_birads\"][0], (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX, 3, (0, 255, 0), 2, cv2.LINE_AA)\n\nplt.imshow(sample_bgr, cmap=\"bone\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_paths = []\nfor i in range(tmp_df.shape[0]):\n    split_type = tmp_df[\"split\"][i]\n    study_id = tmp_df[\"study_id\"][i]\n    image_id = tmp_df[\"image_id\"][i]\n    ann_path = \"/kaggle/working/annotations\" + \"/\" + study_id + \"/\" + image_id + \".txt\"\n    ann_paths.append(ann_path)\n    \ntmp_df[\"ann_paths\"] = ann_paths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = tmp_df[tmp_df.split == \"training\"].reset_index(drop=True)\ntest_df = tmp_df[tmp_df.split == \"test\"].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images, train_annotations = train_df[\"image_path\"], train_df[\"ann_paths\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images, test_annotations = test_df[\"image_path\"], test_df[\"ann_paths\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(train_images) == len(train_annotations)\nassert len(test_images) == len(test_annotations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/images/training\", exist_ok=True) \nos.makedirs(\"/kaggle/working/images/test\", exist_ok=True) \nos.makedirs(\"/kaggle/working/labels/training\", exist_ok=True) \nos.makedirs(\"/kaggle/working/labels/test\", exist_ok=True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def move_files_to_folder(list_of_files, destination_folder, is_image: bool = True):\n    for f in tqdm(list_of_files):\n        try:\n            if is_image:\n                shutil.copy(f, destination_folder)\n            else:\n                # Check if destination folder exists\n                if not os.path.exists(destination_folder):\n                    os.makedirs(destination_folder)\n                \n                # Get the file name from the source path\n                file_name = os.path.basename(f)\n                \n                # Destination path\n                destination_path = os.path.join(destination_folder, file_name)\n                \n                # If destination file exists, append the text to it\n                if os.path.exists(destination_path):\n                    with open(destination_path, \"a\") as dest_file:\n                        dest_file.write(\"\\n\")\n                        with open(f, \"r\") as src_file:\n                            dest_file.write(src_file.read())\n                else:\n                    shutil.move(f, destination_path)\n        except shutil.Error as e:\n            print(f\"Skipping file {f}: {e}\")\n\n# Move the splits into their folders\nmove_files_to_folder(train_images, '/kaggle/working/images/training')\nmove_files_to_folder(test_images, '/kaggle/working/images/test/')\nmove_files_to_folder(train_annotations, '/kaggle/working/labels/training/', is_image=False)\nmove_files_to_folder(test_annotations, '/kaggle/working/labels/test/', is_image=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(os.listdir(\"/kaggle/working/images/training/\")) == len(os.listdir(\"/kaggle/working/labels/training/\"))\nassert len(os.listdir(\"/kaggle/working/images/test/\")) == len(os.listdir(\"/kaggle/working/labels/test/\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5\n!touch /kaggle/working/vindr_mammo.yaml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/yolov5 \n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/vindr_mammo.yaml\n\ntrain: /kaggle/working/images/training/ \nval:  /kaggle/working/images/test/\n\nnames:\n  0: BI-RADS_3\n  1: BI-RADS_4\n  2: BI-RADS_5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!touch /kaggle/working/configyolov5.yaml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/configyolov5.yaml\n\n# YOLOv5 ðŸš€ by Ultralytics, AGPL-3.0 license\n\n# Parameters\nnc: 3 # number of classes\ndepth_multiple: 1.0 # model depth multiple\nwidth_multiple: 1.0 # layer channel multiple\nanchors:\n  - [10, 13, 16, 30, 33, 23] # P3/8\n  - [30, 61, 62, 45, 59, 119] # P4/16\n  - [116, 90, 156, 198, 373, 326] # P5/32\n\n# YOLOv5 v6.0 backbone\nbackbone:\n  # [from, number, module, args]\n  [\n    [-1, 1, Conv, [64, 6, 2, 2]], # 0-P1/2\n    [-1, 1, Conv, [128, 3, 2]], # 1-P2/4\n    [-1, 3, C3, [128]],\n    [-1, 1, Conv, [256, 3, 2]], # 3-P3/8\n    [-1, 6, C3, [256]],\n    [-1, 1, Conv, [512, 3, 2]], # 5-P4/16\n    [-1, 9, C3, [512]],\n    [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32\n    [-1, 3, C3, [1024]],\n    [-1, 1, SPPF, [1024, 5]], # 9\n  ]\n\n# YOLOv5 v6.0 head\nhead: [\n    [-1, 1, Conv, [512, 1, 1]],\n    [-1, 1, nn.Upsample, [None, 2, \"nearest\"]],\n    [[-1, 6], 1, Concat, [1]], # cat backbone P4\n    [-1, 3, C3, [512, False]], # 13\n\n    [-1, 1, Conv, [256, 1, 1]],\n    [-1, 1, nn.Upsample, [None, 2, \"nearest\"]],\n    [[-1, 4], 1, Concat, [1]], # cat backbone P3\n    [-1, 3, C3, [256, False]], # 17 (P3/8-small)\n\n    [-1, 1, Conv, [256, 3, 2]],\n    [[-1, 14], 1, Concat, [1]], # cat head P4\n    [-1, 3, C3, [512, False]], # 20 (P4/16-medium)\n\n    [-1, 1, Conv, [512, 3, 2]],\n    [[-1, 10], 1, Concat, [1]], # cat head P5\n    [-1, 3, C3, [1024, False]], # 23 (P5/32-large)\n\n    [[17, 20, 23], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5)\n  ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/hyp_scratch.yaml\n# Hyperparameters for COCO training from scratch\n# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n\nnc: 3\n\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\nanchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+/- deg)\ntranslate: 0.1  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.5  # image mosaic (probability)\nmixup: 0.0  # image mixup (probability)\ncopy_paste: 0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python train.py --img 512 --batch 16 --epochs 10 --data /kaggle/working/vindr_mammo.yaml --cfg /kaggle/working/configyolov5.yaml --hyp /kaggle/working/hyp_scratch.yaml --name yolov5_deneme_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}